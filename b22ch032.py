# -*- coding: utf-8 -*-
"""B22CH032.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mS3tpiIo9rOuEtrlT-v206JQhpPGCHI0

Importing necessary libraries
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
import tensorflow as tf

"""Loading the Dataset"""

SEED=32
np.random.seed(SEED)

def load_mnist_data():
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
    X = np.concatenate((X_train, X_test), axis=0).reshape(-1, 28*28).astype('float32') / 255.0
    y = np.concatenate((y_train, y_test), axis=0).astype('int')
    return X, y

X, y = load_mnist_data()

def one_hot_encode(y, num_classes):
    encoded = np.zeros((y.shape[0], num_classes))
    encoded[np.arange(y.shape[0]), y] = 1
    return encoded

## I am doing the one-hot encoding because I will be using the categorical cross entropy loss function

y_encoded = one_hot_encode(y, 10)

"""Neural Network Parameters

"""

INPUT_SIZE = 784  # MNIST images are 28x28
HIDDEN_SIZE = 128  # Hidden layer size
OUTPUT_SIZE = 10  # Number of classes
EPOCHS = 25
LEARNING_RATE = 0.01
BATCH_SIZE = 22 # I am from B22

"""Different Weight initialisation Strategy"""

def initialize_weights(input_size, hidden_size, output_size, strategy='random'):
    if strategy == 'xavier':
        W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)
        W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)
    elif strategy == 'he':
        W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)
        W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)
    else:  # Default random initialization
        W1 = np.random.randn(input_size, hidden_size) * 0.01
        W2 = np.random.randn(hidden_size, output_size) * 0.01

    b1 = np.ones((1, hidden_size))
    b2 = np.ones((1, output_size))
    return W1, b1, W2, b2

"""Activation Functions and their derivatives"""

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2

"""Softmax for multiclass classfication"""

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

"""Loss function with L2 regularisation"""

def compute_loss(y_true, y_pred, W1, W2, l2_lambda=0.01):
    cross_entropy_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))
    l2_loss = (l2_lambda / 2) * (np.sum(W1**2) + np.sum(W2**2))
    return cross_entropy_loss + l2_loss

"""Forward Pass"""

def forward_pass(X, W1, b1, W2, b2, activation_func):
    Z1 = np.dot(X, W1) + b1
    if activation_func == 'relu':
        A1 = relu(Z1)
    elif activation_func == 'sigmoid':
        A1 = sigmoid(Z1)
    elif activation_func == 'tanh':
        A1 = tanh(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    cache = {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}
    return A2, cache

"""Backward Pass"""

def backward_pass(X, y_true, cache, W1, W2, activation_func, l2_lambda=0.01):
    m = X.shape[0]
    dZ2 = cache["A2"] - y_true
    dW2 = np.dot(cache["A1"].T, dZ2) / m + l2_lambda * W2
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m
    if activation_func == 'relu':
        dZ1 = np.dot(dZ2, W2.T) * relu_derivative(cache["Z1"])
    elif activation_func == 'sigmoid':
        dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(cache["Z1"])
    elif activation_func == 'tanh':
        dZ1 = np.dot(dZ2, W2.T) * tanh_derivative(cache["Z1"])
    dW1 = np.dot(X.T, dZ1) / m + l2_lambda * W1
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m
    return dW1, db1, dW2, db2

"""Update Parameters"""

def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    return W1, b1, W2, b2

"""Training the model"""

def train_model_with_gradient_descent(X_train, y_train, X_val, y_val, activation_func, weight_init, batch_size, epochs, learning_rate, optimization_mode='mini-batch', early_stopping=False):
    W1, b1, W2, b2 = initialize_weights(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, weight_init)
    history = {"loss": [], "accuracy": []}
    best_loss = float('inf')
    patience = 3  # Early stopping patience
    stop_counter = 0

    for epoch in range(epochs):
        if optimization_mode == 'stochastic':  # SGD (one sample at a time)
            indices = np.arange(X_train.shape[0])
            np.random.shuffle(indices)
            for i in indices:
                X_sample = X_train[i:i+1]  # One sample
                y_sample = y_train[i:i+1]
                y_pred, cache = forward_pass(X_sample, W1, b1, W2, b2, activation_func)
                dW1, db1, dW2, db2 = backward_pass(X_sample, y_sample, cache, W1, W2, activation_func)
                W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)

        elif optimization_mode == 'batch':  # Batch Gradient Descent (entire dataset)
            y_pred, cache = forward_pass(X_train, W1, b1, W2, b2, activation_func)
            dW1, db1, dW2, db2 = backward_pass(X_train, y_train, cache, W1, W2, activation_func)
            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)

        elif optimization_mode == 'mini-batch':  # Mini-Batch Gradient Descent
            indices = np.arange(X_train.shape[0])
            np.random.shuffle(indices)
            X_train = X_train[indices]
            y_train = y_train[indices]
            for i in range(0, X_train.shape[0], batch_size):
                X_batch = X_train[i:i+batch_size]
                y_batch = y_train[i:i+batch_size]
                y_pred, cache = forward_pass(X_batch, W1, b1, W2, b2, activation_func)
                dW1, db1, dW2, db2 = backward_pass(X_batch, y_batch, cache, W1, W2, activation_func)
                W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)

        # Validation loss and accuracy
        y_val_pred, _ = forward_pass(X_val, W1, b1, W2, b2, activation_func)
        val_loss = compute_loss(y_val, y_val_pred, W1, W2)
        val_accuracy = np.mean(np.argmax(y_val_pred, axis=1) == np.argmax(y_val, axis=1))

        history["loss"].append(val_loss)
        history["accuracy"].append(val_accuracy)

        if early_stopping:
            if val_loss < best_loss:
                best_loss = val_loss
                stop_counter = 0
            else:
                stop_counter += 1
                if stop_counter >= patience:
                    print("Early stopping...")
                    break

        print(f"Epoch {epoch+1}/{epochs} - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}")

    return W1, b1, W2, b2, history

"""Experimentation Configurations"""

activations = ['relu', 'sigmoid', 'tanh']
weight_initializations = ['random', 'xavier', 'he']
optimization_modes = ['stochastic', 'mini-batch', 'batch']

"""Actual training on different approaches"""

for activation in activations:
    for weight_init in weight_initializations:
        for optimization_mode in optimization_modes:
            print(f"\nTesting with Activation: {activation}, Weight Init: {weight_init}, Optimization: {optimization_mode}")

            # Splitting data
            X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=SEED)

            # Training the model
            W1, b1, W2, b2, history = train_model_with_gradient_descent(
                X_train, y_train, X_val, y_val,
                activation_func=activation,
                weight_init=weight_init,
                batch_size=BATCH_SIZE,
                epochs=EPOCHS,
                learning_rate=LEARNING_RATE,
                optimization_mode=optimization_mode,
                early_stopping=True
            )

            # Plotting loss and accuracy
            plt.figure(figsize=(12, 5))
            plt.subplot(1, 2, 1)
            plt.plot(history["loss"], label="Validation Loss")
            plt.title(f"Loss Curve - {activation}, {weight_init}, {optimization_mode}")
            plt.xlabel("Epochs")
            plt.ylabel("Loss")
            plt.legend()

            plt.subplot(1, 2, 2)
            plt.plot(history["accuracy"], label="Validation Accuracy")
            plt.title(f"Accuracy Curve - {activation}, {weight_init}, {optimization_mode}")
            plt.xlabel("Epochs")
            plt.ylabel("Accuracy")
            plt.legend()
            plt.show()

"""## Best possible method"""

# Initialize Parameters using He Initialization
def initialize_parameters(input_dim, hidden_units, output_units):
    W1 = np.random.randn(input_dim, hidden_units) * np.sqrt(2. / input_dim)
    b1 = np.zeros((1, hidden_units))
    W2 = np.random.randn(hidden_units, output_units) * np.sqrt(2. / hidden_units)
    b2 = np.zeros((1, output_units))
    return W1, b1, W2, b2


# Activation Functions
def relu(x):
    return np.maximum(0, x)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# Forward Pass
def forward_pass(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    cache = {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}
    return A2, cache

# Backward Pass
def backward_pass(X, y, cache, W1, b1, W2, b2, learning_rate):
    m = X.shape[0]
    A1 = cache["A1"]
    A2 = cache["A2"]

    dZ2 = A2 - y
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    dZ1 = np.dot(dZ2, W2.T) * (A1 > 0)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    # Update parameters
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2

    return W1, b1, W2, b2

def learning_rate_scheduler(initial_lr, patience, factor, epoch, val_losses):
    if len(val_losses) > patience and val_losses[-1] > val_losses[-patience - 1]:
        return max(initial_lr * (factor ** (epoch // patience)), 1e-6)
    return initial_lr

def calculate_accuracy(y_true, y_pred):
    """
    Calculate accuracy given true labels and predicted probabilities.
    """
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_true, axis=1)
    accuracy = np.mean(y_pred_classes == y_true_classes)
    return accuracy

def cross_entropy_loss(y_true, y_pred):
    """
    Compute the categorical cross-entropy loss.
    """
    epsilon = 1e-15  # To prevent log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities
    loss = -np.sum(y_true * np.log(y_pred), axis=1)
    return loss

def train(X_train, y_train, X_val, y_val, epochs, batch_size, initial_lr, patience, factor):
    input_dim = X_train.shape[1]
    hidden_units = 128  # Example number of hidden units
    output_units = y_train.shape[1]

    # Initialize parameters
    W1, b1, W2, b2 = initialize_parameters(input_dim, hidden_units, output_units)

    best_val_loss = float('inf')
    no_improvement = 0
    learning_rate = initial_lr

    # Lists to store metrics for plotting
    val_losses = []
    val_accuracies = []

    for epoch in range(epochs):
        # Mini-batch gradient descent
        for i in range(0, X_train.shape[0], batch_size):
            X_batch = X_train[i:i + batch_size]
            y_batch = y_train[i:i + batch_size]

            # Forward pass
            y_pred, cache = forward_pass(X_batch, W1, b1, W2, b2)

            # Backward pass
            W1, b1, W2, b2 = backward_pass(X_batch, y_batch, cache, W1, b1, W2, b2, learning_rate)

        # Calculate validation loss and accuracy
        y_val_pred, _ = forward_pass(X_val, W1, b1, W2, b2)
        val_loss = np.mean(cross_entropy_loss(y_val, y_val_pred))
        val_accuracy = calculate_accuracy(y_val, y_val_pred)

        # Store metrics for plotting
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        print(f"Epoch {epoch + 1}/{epochs}: Validation Loss = {val_loss:.4f}, Validation Accuracy = {val_accuracy:.4f}")

        # Learning rate scheduling
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            no_improvement = 0
        else:
            no_improvement += 1
            if no_improvement >= patience:
                learning_rate *= factor
                no_improvement = 0
                print(f"Reduced learning rate to {learning_rate:.6f}")

    return W1, b1, W2, b2, val_losses, val_accuracies

X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=SEED)

initial_lr = 0.1
patience = 5
factor = 0.5
epochs = 25
batch_size = 22

W1, b1, W2, b2, val_losses, val_accuracies = train(X_train, y_train, X_val, y_val, epochs, batch_size, initial_lr, patience, factor)

# Plot Validation Loss
plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', marker='o')
plt.title("Validation Loss Over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.grid()
plt.legend()
plt.show()

# Plot Validation Accuracy
plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy', marker='o', color='orange')
plt.title("Validation Accuracy Over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.grid()
plt.legend()
plt.show()

# Confusion Matrix
y_val_pred, _ = forward_pass(X_val, W1, b1, W2, b2)
y_val_pred_classes = np.argmax(y_val_pred, axis=1)
y_val_true_classes = np.argmax(y_val, axis=1)

cm = confusion_matrix(y_val_true_classes, y_val_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

